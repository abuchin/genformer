{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fce1a7-ac1b-4f1c-8aae-f7f4c3f25420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 02:53:20.481741: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 02:53:20.655513: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-24 02:53:20.655548: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-04-24 02:53:21.572796: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 02:53:21.572939: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-24 02:53:21.572953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "import re\n",
    "import argparse\n",
    "import collections\n",
    "import gzip\n",
    "import math\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "#silence_tensorflow()\n",
    "os.environ['TPU_LOAD_LIBRARY']='0'\n",
    "os.environ['TF_ENABLE_EAGER_CLIENT_STREAMING_ENQUEUE']='False'\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import strings as tfs\n",
    "from tensorflow.keras import mixed_precision\n",
    "from scipy.stats.stats import pearsonr  \n",
    "from scipy.stats.stats import spearmanr  \n",
    "## custom modules\n",
    "import src.aformer_atac as aformer\n",
    "#import src.aformer_TF as aformer\n",
    "from src.layers.layers import *\n",
    "import src.metrics as metrics\n",
    "from src.optimizers import *\n",
    "import src.schedulers as schedulers\n",
    "import src.utils as utils\n",
    "\n",
    "import training_utils_atac as training_utils\n",
    "\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f91bfd-ee1f-428d-8c76-aa0bcdd8f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 02:53:23.010331: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-24 02:53:23.010381: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-24 02:53:23.010405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (tpu-genformer-v2-6): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 02:53:23.337133: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "2023-04-24 02:53:23.362744: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://localhost:54236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: node-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-2')\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.FILE\n",
    "    options.deterministic=False\n",
    "    #options.experimental_threading.max_intra_op_parallelism = 1\n",
    "    mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    #options.num_devices = 64\n",
    "\n",
    "    BATCH_SIZE_PER_REPLICA = 1\n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb91825-4d60-46de-b695-f446906af355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "    def one_hot(sequence):\n",
    "        '''\n",
    "        convert input string tensor to one hot encoded\n",
    "        will replace all N character with 0 0 0 0\n",
    "        '''\n",
    "        vocabulary = tf.constant(['A', 'C', 'G', 'T'])\n",
    "        mapping = tf.constant([0, 1, 2, 3])\n",
    "\n",
    "        init = tf.lookup.KeyValueTensorInitializer(keys=vocabulary,\n",
    "                                                   values=mapping)\n",
    "        table = tf.lookup.StaticHashTable(init, default_value=0)\n",
    "\n",
    "        input_characters = tfs.upper(tfs.unicode_split(sequence, 'UTF-8'))\n",
    "\n",
    "        out = tf.one_hot(table.lookup(input_characters), \n",
    "                          depth = 4, \n",
    "                          dtype=tf.float32)\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "g = tf.random.Generator.from_seed(1)\n",
    "\n",
    "with strategy.scope():\n",
    "    list_files = tf.io.gfile.glob(\"gs://picard-testing-176520/genformer_atac_pretrain/262k/genformer_atac_pretrain_globalacc_conv_rpgc_test_holdout/peak_atlas_with_negatives/*.tfr\")\n",
    "\n",
    "\n",
    "    files = tf.data.Dataset.list_files(list_files)\n",
    "\n",
    "    def deserialize_val(serialized_example,\n",
    "                       input_length,\n",
    "                       max_shift,\n",
    "                       output_length_ATAC,\n",
    "                       output_length,\n",
    "                       crop_size,\n",
    "                       output_res,\n",
    "                       #seq_mask_dropout,\n",
    "                       atac_mask_dropout,\n",
    "                       mask_size,\n",
    "                       log_atac,\n",
    "                       use_atac,\n",
    "                       use_seq,\n",
    "                        g):\n",
    "        \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "        ## parse out feature map\n",
    "        feature_map = {\n",
    "            'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "            'atac': tf.io.FixedLenFeature([], tf.string),\n",
    "            'tss_tokens': tf.io.FixedLenFeature([], tf.string),\n",
    "            'peaks': tf.io.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        ### stochastic sequence shift and gaussian noise\n",
    "\n",
    "        seq_shift=5\n",
    "        stupid_random_seed = g.uniform([], 0, 10000000,dtype=tf.int32)\n",
    "        input_seq_length = input_length + max_shift\n",
    "\n",
    "        ## now parse out the actual data\n",
    "        data = tf.io.parse_example(serialized_example, feature_map)\n",
    "        sequence = one_hot(tf.strings.substr(data['sequence'],\n",
    "                                     seq_shift,input_length))\n",
    "        atac = tf.ensure_shape(tf.io.parse_tensor(data['atac'],\n",
    "                                                  out_type=tf.float32),\n",
    "                               [output_length_ATAC,1])\n",
    "        peaks = tf.ensure_shape(tf.io.parse_tensor(data['peaks'],\n",
    "                                                  out_type=tf.int32),\n",
    "                               [output_length])\n",
    "        peaks = tf.expand_dims(peaks,axis=1)\n",
    "        peaks_crop = tf.slice(peaks,\n",
    "                         [crop_size,0],\n",
    "                         [output_length-2*crop_size,-1])\n",
    "\n",
    "\n",
    "        center = (output_length-2*crop_size)//2\n",
    "        ### here set up masking of one of the peaks\n",
    "        mask_indices_temp = tf.where(peaks_crop[:,0] > 0)[:,0]\n",
    "        ridx = tf.concat([tf.constant([center],dtype=tf.int64)],axis=0)   ### concatenate the middle in case theres no peaks\n",
    "        mask_indices=[[ridx[0]-3+crop_size],[ridx[0]-2+crop_size],\n",
    "                      [ridx[0]-1+crop_size],[ridx[0]+crop_size],[ridx[0]+1+crop_size],\n",
    "                      [ridx[0]+2+crop_size],[ridx[0]+3+crop_size],\n",
    "                      [ridx[0]+4+crop_size]]\n",
    "\n",
    "        st=tf.SparseTensor(\n",
    "            indices=mask_indices,\n",
    "            values=[1.0]*len(mask_indices),\n",
    "            dense_shape=[output_length])\n",
    "        dense_peak_mask=tf.sparse.to_dense(st)\n",
    "        dense_peak_mask_store = dense_peak_mask\n",
    "        dense_peak_mask=1.0-dense_peak_mask\n",
    "        dense_peak_mask = tf.expand_dims(dense_peak_mask,axis=1)\n",
    "\n",
    "        atac_target = atac ## store the target\n",
    "\n",
    "        ### here set up the ATAC masking\n",
    "        num_mask_bins = mask_size // output_res\n",
    "        out_length_cropped = output_length-2*crop_size\n",
    "        edge_append = tf.ones((crop_size,1),dtype=tf.float32)\n",
    "        atac_mask = tf.ones(out_length_cropped // num_mask_bins,dtype=tf.float32)\n",
    "        atac_mask=tf.nn.experimental.stateless_dropout(atac_mask,\n",
    "                                                  rate=(atac_mask_dropout),\n",
    "                                                  seed=[stupid_random_seed+16,stupid_random_seed+10]) / (1. / (1.0-(atac_mask_dropout))) \n",
    "        atac_mask = tf.expand_dims(atac_mask,axis=1)\n",
    "        atac_mask = tf.tile(atac_mask, [1,num_mask_bins])\n",
    "        atac_mask = tf.reshape(atac_mask, [-1])\n",
    "        atac_mask = tf.expand_dims(atac_mask,axis=1)\n",
    "        atac_mask_store = 1.0 - atac_mask\n",
    "        full_atac_mask = tf.concat([edge_append,atac_mask,edge_append],axis=0)\n",
    "        full_comb_mask = tf.math.floor((dense_peak_mask+full_atac_mask)/2)\n",
    "        full_comb_mask_store = 1.0 - full_comb_mask\n",
    "        full_comb_mask_store = full_comb_mask_store[crop_size:-crop_size,:]\n",
    "        tiling_req = output_length_ATAC // output_length\n",
    "        full_comb_mask = tf.expand_dims(tf.reshape(tf.tile(full_comb_mask, [1,tiling_req]),[-1]),axis=1)\n",
    "        masked_atac = atac * full_comb_mask\n",
    "\n",
    "        ### now that we have masked specific tokens by setting them to 0, we want to randomly add wrong tokens to these positions\n",
    "        ## first, invert the mask\n",
    "        random_shuffled_tokens= tf.random.experimental.stateless_shuffle(atac,seed=[10,stupid_random_seed+10])\n",
    "        masked_atac = masked_atac + (1.0-full_comb_mask)*random_shuffled_tokens\n",
    "\n",
    "        if log_atac: \n",
    "            masked_atac = tf.math.log1p(masked_atac)\n",
    "\n",
    "        diff = tf.math.sqrt(tf.nn.relu(masked_atac - 100.0 * tf.ones(masked_atac.shape)))\n",
    "        masked_atac = tf.clip_by_value(masked_atac, clip_value_min=0.0, clip_value_max=100.0) + diff\n",
    "\n",
    "        atac_out = tf.reduce_sum(tf.reshape(atac_target, [-1,tiling_req]),axis=1,keepdims=True)\n",
    "        diff = tf.math.sqrt(tf.nn.relu(atac_out - 2500.0 * tf.ones(atac_out.shape)))\n",
    "        atac_out = tf.clip_by_value(atac_out, clip_value_min=0.0, clip_value_max=2500.0) + diff\n",
    "        atac_out = tf.slice(atac_out,\n",
    "                            [crop_size,0],\n",
    "                            [output_length-2*crop_size,-1])\n",
    "\n",
    "        peaks_gathered = tf.reduce_max(tf.reshape(peaks_crop, [(output_length-2*crop_size) // 2, -1]),\n",
    "                                       axis=1,keepdims=True)\n",
    "        mask_gathered = tf.reduce_max(tf.reshape(full_comb_mask_store, [(output_length-2*crop_size) // 2, -1]),\n",
    "                                       axis=1,keepdims=True)\n",
    "\n",
    "        random_shuffled_tokens= tf.random.experimental.stateless_shuffle(atac,\n",
    "                                                                         seed=[11,stupid_random_seed+11])\n",
    "        if not use_atac:\n",
    "            masked_atac = random_shuffled_tokens\n",
    "        if not use_seq:\n",
    "            sequence = tf.random.experimental.stateless_shuffle(sequence,\n",
    "                                                                seed=[1,stupid_random_seed+12])\n",
    "\n",
    "\n",
    "        return {'sequence': tf.ensure_shape(sequence,\n",
    "                                            [input_length,4]),\n",
    "                'atac': tf.ensure_shape(masked_atac,\n",
    "                                        [output_length_ATAC,1]),\n",
    "                'mask': tf.ensure_shape(full_comb_mask_store,\n",
    "                                        [output_length-crop_size*2,1]),\n",
    "                'mask_gathered': tf.ensure_shape(mask_gathered,\n",
    "                                        [(output_length-crop_size*2)//2,1]),\n",
    "                'peaks': tf.ensure_shape(peaks_gathered,\n",
    "                                          [(output_length-2*crop_size) // 2,1]),\n",
    "                'target': tf.ensure_shape(atac_out,\n",
    "                                          [output_length-crop_size*2,1])}\n",
    "\n",
    "\n",
    "    files = tf.data.Dataset.list_files(list_files)\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(files,\n",
    "                                      compression_type='ZLIB',\n",
    "                                      num_parallel_reads=4)\n",
    "    dataset = dataset.with_options(options)\n",
    "    dataset = dataset.map(lambda record: deserialize_val(record,\n",
    "                                                        262144,\n",
    "                                                        10,\n",
    "                                                        65536,\n",
    "                                                        2048,\n",
    "                                                        256,\n",
    "                                                        128,\n",
    "                                                         #seq_mask_dropout,\n",
    "                                                        0.0,\n",
    "                                                        512,\n",
    "                                                        True,\n",
    "                                                       True,\n",
    "                                                       True,\n",
    "                                                        g),\n",
    "                  deterministic=False,\n",
    "                  num_parallel_calls=4)\n",
    "\n",
    "    dataset = dataset.batch(64,drop_remainder=True).prefetch(tf.data.AUTOTUNE).repeat(1)\n",
    "\n",
    "    test_dist = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    test_it = iter(test_dist)\n",
    "    test_it_build = iter(test_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0305b462-08ba-4a3a-a07a-f54475f54b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran test input\n",
      "loaded weights\n"
     ]
    }
   ],
   "source": [
    "#with strategy.scope():\n",
    "with strategy.scope():\n",
    "    model = aformer.aformer(kernel_transformation='relu_kernel_transformation',\n",
    "                            dropout_rate=0.30,\n",
    "                            pointwise_dropout_rate=0.10,\n",
    "                            input_length=262144,\n",
    "                            output_length=2048,\n",
    "                            final_output_length=1536,\n",
    "                            num_heads=8,\n",
    "                            numerical_stabilizer=0.0000001,\n",
    "                            nb_random_features=256,\n",
    "                            max_seq_length=2048,\n",
    "                            rel_pos_bins=2048,\n",
    "                            norm=True,\n",
    "                            normalize = True,\n",
    "                            BN_momentum=0.90,\n",
    "                            use_rot_emb = True,\n",
    "                            use_mask_pos = False,\n",
    "                            num_transformer_layers=6,\n",
    "                            inits_type=\"enformer_performer\",\n",
    "                            load_init=True,\n",
    "                            stable_variant=False,\n",
    "                            freeze_conv_layers=False,\n",
    "                            filter_list_seq=[768,896,1024,1152,1280,1536],\n",
    "                            filter_list_atac=[32,64],\n",
    "                            output_heads=[\"human\",\"mouse\",\"rhesus\",\"rat\"],\n",
    "                            learnable_PE=True)\n",
    "\n",
    "\n",
    "    def build_step(iterator): #input_batch, model, optimizer, organism, gradient_clip):\n",
    "        @tf.function(jit_compile=True)\n",
    "        def test_step(inputs):\n",
    "            sequence=tf.cast(inputs['sequence'],dtype=tf.bfloat16)\n",
    "            atac=tf.cast(inputs['atac'],dtype=tf.bfloat16)\n",
    "            target=tf.cast(inputs['target'],dtype=tf.float32)\n",
    "            #global_acc=tf.cast(inputs['global_acc'],dtype=tf.bfloat16)         \n",
    "            input_tuple = sequence,atac#,global_acc\n",
    "\n",
    "            output = model(input_tuple,\n",
    "                           training=False)\n",
    "\n",
    "        for _ in tf.range(1): ## for loop within @tf.fuction for improved TPU performance\n",
    "            strategy.run(test_step, args=(next(iterator),))\n",
    "            \n",
    "            \n",
    "build_step(test_it_build)\n",
    "print('ran test input')\n",
    "#model.load_weights(\"gs://picard-testing-176520/genformer_atac_pretrain/models/aformer_hg_262k_load-True_LR-0.01_T-7_D-0.3_2023-04-13_15:39:43/final/saved_model\")\n",
    "model.load_weights(\"gs://picard-testing-176520/genformer_atac_pretrain/models/aformer_hg_mm_rm_rat_262k_load-True_LR-0.01_T-6_D-0.3_2023-04-22_02:47:21/iteration_13/saved_model\")\n",
    "print('loaded weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71878139-2569-40ab-a0b9-744b82a1e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    metric_dict = {}\n",
    "    metric_dict[\"corr_stats_atac\"] = metrics.correlation_stats_gene_centered(name='corr_stats')\n",
    "    metric_dict[\"corr_stats_peaks\"] = metrics.correlation_stats_gene_centered(name='corr_stats')\n",
    "    def dist_test_step(iterator):\n",
    "        @tf.function(jit_compile=True)\n",
    "        def test_step(inputs):\n",
    "            sequence=tf.cast(inputs['sequence'],dtype=tf.bfloat16)\n",
    "            target=tf.cast(inputs['target'],dtype=tf.float32)\n",
    "            atac=tf.cast(inputs['atac'],dtype=tf.bfloat16)\n",
    "            mask=tf.cast(inputs['mask'],dtype=tf.int32)\n",
    "            mask_gathered=tf.cast(inputs['mask_gathered'],dtype=tf.int32)\n",
    "            peaks=tf.cast(inputs['peaks'],dtype=tf.float32)\n",
    "            \n",
    "            input_tuple = sequence,atac#,global_acc\n",
    "\n",
    "            output_profile,output_peaks = model(input_tuple,\n",
    "                                                training=False)\n",
    "            output_profile = tf.cast(output_profile['human'],dtype=tf.float32) # ensure cast to float32\n",
    "            output_peaks = tf.cast(output_peaks['human'],dtype=tf.float32)\n",
    "            \n",
    "            mask_indices = tf.where(mask[0,:,0] == 1)[:,0]\n",
    "            \n",
    "            target_atac = tf.gather(target[:,:,0], mask_indices,axis=1)\n",
    "            output_atac = tf.gather(output_profile[:,:,0], mask_indices,axis=1)\n",
    "            \n",
    "            \n",
    "            mask_gather_indices = tf.where(mask_gathered[0,:,0] == 1)[:,0]\n",
    "            target_peaks = tf.gather(peaks[:,:,0], mask_gather_indices,axis=1)\n",
    "            output_peaks = tf.gather(output_peaks[:,:,0], mask_gather_indices,axis=1)\n",
    "\n",
    "            #dummy_var = tf.ones(target_atac.shape,dtype=tf.int32)\n",
    "            \n",
    "            return target_atac, output_atac, target_peaks, output_peaks#,dummy_var\n",
    "            \n",
    "        \n",
    "        ta_pred_atac = tf.TensorArray(tf.float32, size=0, dynamic_size=True,clear_after_read=False) # tensor array to store preds\n",
    "        ta_true_atac = tf.TensorArray(tf.float32, size=0, dynamic_size=True,clear_after_read=False) # tensor array to store vals\n",
    "        ta_pred_peaks = tf.TensorArray(tf.float32, size=0, dynamic_size=True,clear_after_read=False) # tensor array to store preds\n",
    "        ta_true_peaks = tf.TensorArray(tf.float32, size=0, dynamic_size=True,clear_after_read=False)  \n",
    "        ta_dummy_pk = tf.TensorArray(tf.int32, size=0, dynamic_size=True,clear_after_read=False) \n",
    "        ta_dummy_atac = tf.TensorArray(tf.int32, size=0, dynamic_size=True,clear_after_read=False) \n",
    "            \n",
    "        for _ in tf.range(2017): ## for loop within @tf.fuction for improved TPU performance\n",
    "            pred_atac_rep,true_atac_rep, pred_peaks_rep,true_peaks_rep = \\\n",
    "                strategy.run(test_step,\n",
    "                             args=(next(iterator),))\n",
    "            \n",
    "            #print(pred_atac_rep)\n",
    "            \n",
    "            pred_atac_reshape = tf.reshape(strategy.gather(pred_atac_rep, axis=1), [-1]) # reshape to 1D\n",
    "            true_atac_reshape = tf.reshape(strategy.gather(true_atac_rep, axis=1), [-1])\n",
    "            pred_peaks_reshape = tf.reshape(strategy.gather(pred_peaks_rep, axis=1), [-1]) # reshape to 1D\n",
    "            true_peaks_reshape = tf.reshape(strategy.gather(true_peaks_rep, axis=1), [-1])\n",
    "            \n",
    "            dummy_reshape_pk = tf.ones(true_peaks_reshape.shape,dtype=tf.int32)#tf.reshape(strategy.gather(dummy_int, axis=0), [-1])\n",
    "            dummy_reshape_atac = tf.ones(true_peaks_reshape.shape,dtype=tf.int32)\n",
    "\n",
    "            ta_pred_atac = ta_pred_atac.write(_, pred_atac_reshape)\n",
    "            ta_true_atac = ta_true_atac.write(_, true_atac_reshape)\n",
    "            ta_pred_peaks = ta_pred_peaks.write(_, pred_peaks_reshape)\n",
    "            ta_true_peaks = ta_true_peaks.write(_, true_peaks_reshape)\n",
    "            \n",
    "            ta_dummy_pk = ta_dummy_pk.write(_, dummy_reshape_pk)\n",
    "            ta_dummy_atac = ta_dummy_atac.write(_, dummy_reshape_atac)\n",
    "            \n",
    "        metric_dict[\"corr_stats_atac\"].update_state(ta_pred_atac.concat(),\n",
    "                                                    ta_true_atac.concat(),\n",
    "                                                    ta_dummy_atac.concat(),\n",
    "                                                    ta_dummy_atac.concat())\n",
    "        metric_dict[\"corr_stats_peaks\"].update_state(ta_pred_peaks.concat(),\n",
    "                                                     ta_true_peaks.concat(),\n",
    "                                                     ta_dummy_pk.concat(),\n",
    "                                                     ta_dummy_pk.concat())\n",
    "        ta_pred_atac.close()\n",
    "        ta_true_atac.close()\n",
    "        ta_pred_peaks.close()\n",
    "        ta_true_peaks.close()\n",
    "        ta_dummy_pk.close()\n",
    "        ta_dummy_atac.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510472c2-3278-4a5a-84e0-f1c8d0822fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    for k in range(1):\n",
    "        dist_test_step(test_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0532a8-303c-48d3-884f-fed17a487ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.metrics import auc, plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_trues_peaks = metric_dict['corr_stats_peaks'].result()['y_trues'].numpy()\n",
    "y_preds_peaks = metric_dict['corr_stats_peaks'].result()['y_preds'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f944f-46b3-4025-a412-f6689ea1b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_trues_peaks, y_preds_peaks)\n",
    "\n",
    "#create precision recall curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='purple')\n",
    "\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "\n",
    "#display plot\n",
    "plt.savefig(\"PR_curve_FULL.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(auc_precision_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8036f9d9-e24a-43df-bb95-cd12665697ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_peaks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2704a5fe-7eff-4ba2-b1d7-4b226492d671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597978de-b857-4d61-95f4-42b0e5ca9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_trues_peaks, y_preds_peaks)\n",
    "\n",
    "#create precision recall curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='purple')\n",
    "\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "\n",
    "#display plot\n",
    "plt.show()\n",
    "\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(auc_precision_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febaf41a-0d1a-4a62-b4e9-aa6c744c93d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate precision and recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_trues_peaks, y_preds_peaks)\n",
    "\n",
    "#create precision recall curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='purple')\n",
    "\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "\n",
    "#display plot\n",
    "plt.show()\n",
    "\n",
    "auc_precision_recall = auc(recall, precision)\n",
    "print(auc_precision_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd041675-44c8-49b2-bb30-687134f1a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues_atac = metric_dict['corr_stats_atac'].result()['y_trues'].numpy()\n",
    "y_preds_atac = metric_dict['corr_stats_atac'].result()['y_preds'].numpy()\n",
    "\n",
    "y_trues_atac = np.log2(1.0+y_trues_atac)\n",
    "y_preds_atac = np.log2(1.0+y_preds_atac)\n",
    "\n",
    "idx = np.random.choice(np.arange(len(y_trues_atac)), 15000, replace=False)\n",
    "\n",
    "data = np.vstack([y_trues_atac[idx],\n",
    "                  y_preds_atac[idx]])\n",
    "\n",
    "min_true = min(y_trues_atac[idx])\n",
    "max_true = max(y_trues_atac[idx])\n",
    "\n",
    "min_pred = min(y_preds_atac[idx])\n",
    "max_pred = max(y_preds_atac[idx])\n",
    "\n",
    "fig_overall,ax_overall=plt.subplots(figsize=(6,6))\n",
    "kernel = stats.gaussian_kde(data)(data)\n",
    "sns.scatterplot(\n",
    "    x=y_trues_atac[idx],\n",
    "    y=y_preds_atac[idx],\n",
    "    c=kernel,\n",
    "    cmap=\"viridis\")\n",
    "ax_overall.set_xlim(min_true,max_true)\n",
    "ax_overall.set_ylim(min_pred,max_pred)\n",
    "plt.xlabel(\"log-true\")\n",
    "plt.ylabel(\"log-pred\")\n",
    "plt.title(\"overall ATAC bin level correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18d0f3-1832-4dca-9016-8c8a7f58b719",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_atac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571dbf4-3fd6-49fd-89d0-75c0fdc8bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "50 * 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264baaa4-b3ad-499b-83df-ebaa9d3eb88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "    def one_hot(sequence):\n",
    "        '''\n",
    "        convert input string tensor to one hot encoded\n",
    "        will replace all N character with 0 0 0 0\n",
    "        '''\n",
    "        vocabulary = tf.constant(['A', 'C', 'G', 'T'])\n",
    "        mapping = tf.constant([0, 1, 2, 3])\n",
    "\n",
    "        init = tf.lookup.KeyValueTensorInitializer(keys=vocabulary,\n",
    "                                                   values=mapping)\n",
    "        table = tf.lookup.StaticHashTable(init, default_value=0)\n",
    "\n",
    "        input_characters = tfs.upper(tfs.unicode_split(sequence, 'UTF-8'))\n",
    "\n",
    "        out = tf.one_hot(table.lookup(input_characters), \n",
    "                          depth = 4, \n",
    "                          dtype=tf.float32)\n",
    "        return out\n",
    "\n",
    "    \n",
    "list_files = tf.io.gfile.glob(\"gs://picard-testing-176520/genformer_atac_pretrain/262k/genformer_atac_pretrain_globalacc_conv_rpgc_test_holdout/test/*.tfr\")\n",
    "\n",
    "files_list = []\n",
    "for list_file in list_files:\n",
    "    files = tf.data.Dataset.list_files(list_file)\n",
    "\n",
    "    g = tf.random.Generator.from_seed(42)\n",
    "    h = tf.random.Generator.from_seed(43)\n",
    "\n",
    "    with strategy.scope():\n",
    "\n",
    "            def deserialize_val(serialized_example,\n",
    "                               input_length,\n",
    "                               max_shift,\n",
    "                               output_length_ATAC,\n",
    "                               output_length,\n",
    "                               crop_size,\n",
    "                               output_res,\n",
    "                               #seq_mask_dropout,\n",
    "                               atac_mask_dropout,\n",
    "                               mask_size,\n",
    "                               log_atac,\n",
    "                               use_atac,\n",
    "                               use_seq,\n",
    "                                g,h):\n",
    "                \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "                ## parse out feature map\n",
    "                feature_map = {\n",
    "                    'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'atac': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'tss_tokens': tf.io.FixedLenFeature([], tf.string),\n",
    "                    'peaks': tf.io.FixedLenFeature([], tf.string)\n",
    "                }\n",
    "                ### stochastic sequence shift and gaussian noise\n",
    "\n",
    "                seq_shift=5\n",
    "                stupid_random_seed = g.uniform([], 0, 10000000,dtype=tf.int32)\n",
    "                stupid_random_seed2 = h.uniform([], 0, 10000000,dtype=tf.int32)\n",
    "                input_seq_length = input_length + max_shift\n",
    "\n",
    "                ## now parse out the actual data\n",
    "                data = tf.io.parse_example(serialized_example, feature_map)\n",
    "                sequence = one_hot(tf.strings.substr(data['sequence'],\n",
    "                                             seq_shift,input_length))\n",
    "                atac = tf.ensure_shape(tf.io.parse_tensor(data['atac'],\n",
    "                                                          out_type=tf.float32),\n",
    "                                       [output_length_ATAC,1])\n",
    "                peaks = tf.ensure_shape(tf.io.parse_tensor(data['peaks'],\n",
    "                                                          out_type=tf.int32),\n",
    "                                       [output_length])\n",
    "                peaks = tf.expand_dims(peaks,axis=1)\n",
    "                peaks_crop = tf.slice(peaks,\n",
    "                                 [crop_size,0],\n",
    "                                 [output_length-2*crop_size,-1])\n",
    "\n",
    "\n",
    "                center = (output_length-2*crop_size)//2\n",
    "                ### here set up masking of one of the peaks\n",
    "                mask_indices_temp = tf.where(peaks_crop[:,0] > 0)[:,0]\n",
    "                ridx = tf.concat([tf.random.experimental.stateless_shuffle(mask_indices_temp,seed=[11,stupid_random_seed+1]),\n",
    "                                  tf.constant([center],dtype=tf.int64)],axis=0)   ### concatenate the middle in case theres no peaks\n",
    "                mask_indices=[[ridx[0]-4+crop_size],[ridx[0]-3+crop_size],[ridx[0]-2+crop_size],\n",
    "                              [ridx[0]-1+crop_size],[ridx[0]+crop_size],[ridx[0]+1+crop_size],\n",
    "                              [ridx[0]+2+crop_size],[ridx[0]+3+crop_size]]\n",
    "\n",
    "                st=tf.SparseTensor(\n",
    "                    indices=mask_indices,\n",
    "                    values=[1.0]*len(mask_indices),\n",
    "                    dense_shape=[output_length])\n",
    "                dense_peak_mask=tf.sparse.to_dense(st)\n",
    "                dense_peak_mask_store = dense_peak_mask\n",
    "                dense_peak_mask=1.0-dense_peak_mask\n",
    "                dense_peak_mask = tf.expand_dims(dense_peak_mask,axis=1)\n",
    "\n",
    "                atac_target = atac ## store the target\n",
    "\n",
    "                ### here set up the ATAC masking\n",
    "                num_mask_bins = mask_size // output_res\n",
    "                out_length_cropped = output_length-2*crop_size\n",
    "                edge_append = tf.ones((crop_size,1),dtype=tf.float32)\n",
    "                atac_mask = tf.ones(out_length_cropped // num_mask_bins,dtype=tf.float32)\n",
    "                atac_mask=tf.nn.experimental.stateless_dropout(atac_mask,\n",
    "                                                          rate=(atac_mask_dropout),\n",
    "                                                          seed=[stupid_random_seed+16,stupid_random_seed+10]) / (1. / (1.0-(atac_mask_dropout))) \n",
    "                atac_mask = tf.expand_dims(atac_mask,axis=1)\n",
    "                atac_mask = tf.tile(atac_mask, [1,num_mask_bins])\n",
    "                atac_mask = tf.reshape(atac_mask, [-1])\n",
    "                atac_mask = tf.expand_dims(atac_mask,axis=1)\n",
    "                atac_mask_store = 1.0 - atac_mask\n",
    "                full_atac_mask = tf.concat([edge_append,atac_mask,edge_append],axis=0)\n",
    "                full_comb_mask = tf.math.floor((dense_peak_mask + full_atac_mask)/2)\n",
    "                full_comb_mask_store = 1.0 - full_comb_mask\n",
    "                full_comb_mask_store = full_comb_mask_store[crop_size:-crop_size,:]\n",
    "                tiling_req = output_length_ATAC // output_length\n",
    "                full_comb_mask = tf.expand_dims(tf.reshape(tf.tile(full_comb_mask, [1,tiling_req]),[-1]),axis=1)\n",
    "                masked_atac = atac * full_comb_mask\n",
    "\n",
    "                ### now that we have masked specific tokens by setting them to 0, we want to randomly add wrong tokens to these positions\n",
    "                ## first, invert the mask\n",
    "                random_shuffled_tokens= tf.random.experimental.stateless_shuffle(atac,seed=[10,stupid_random_seed+10])\n",
    "                masked_atac = masked_atac + (1.0-full_comb_mask)*random_shuffled_tokens\n",
    "\n",
    "                if log_atac: \n",
    "                    masked_atac = tf.math.log1p(masked_atac)\n",
    "\n",
    "                diff = tf.math.sqrt(tf.nn.relu(masked_atac - 100.0 * tf.ones(masked_atac.shape)))\n",
    "                masked_atac = tf.clip_by_value(masked_atac, clip_value_min=0.0, clip_value_max=100.0) + diff\n",
    "\n",
    "                atac_out = tf.reduce_sum(tf.reshape(atac_target, [-1,tiling_req]),axis=1,keepdims=True)\n",
    "                diff = tf.math.sqrt(tf.nn.relu(atac_out - 2500.0 * tf.ones(atac_out.shape)))\n",
    "                atac_out = tf.clip_by_value(atac_out, clip_value_min=0.0, clip_value_max=2500.0) + diff\n",
    "                atac_out = tf.slice(atac_out,\n",
    "                                    [crop_size,0],\n",
    "                                    [output_length-2*crop_size,-1])\n",
    "\n",
    "                peaks_gathered = tf.reduce_max(tf.reshape(peaks_crop, [(output_length-2*crop_size) // 2, -1]),\n",
    "                                               axis=1,keepdims=True)\n",
    "                mask_gathered = tf.reduce_max(tf.reshape(full_comb_mask_store, [(output_length-2*crop_size) // 2, -1]),\n",
    "                                               axis=1,keepdims=True)\n",
    "\n",
    "                random_shuffled_tokens= tf.random.experimental.stateless_shuffle(atac,\n",
    "                                                                                 seed=[11,stupid_random_seed+11])\n",
    "                if not use_atac:\n",
    "                    masked_atac = random_shuffled_tokens\n",
    "                if not use_seq:\n",
    "                    sequence = tf.random.experimental.stateless_shuffle(sequence,\n",
    "                                                                        seed=[12,stupid_random_seed+12])\n",
    "\n",
    "\n",
    "                mask_seed = tf.cast(full_comb_mask_store,dtype=tf.int32)*stupid_random_seed + stupid_random_seed2\n",
    "                masked_gathered_seed = tf.cast(mask_gathered,dtype=tf.int32)*stupid_random_seed + stupid_random_seed2\n",
    "\n",
    "                return {'sequence': tf.ensure_shape(sequence,\n",
    "                                                    [input_length,4]),\n",
    "                        'atac': tf.ensure_shape(masked_atac,\n",
    "                                                [output_length_ATAC,1]),\n",
    "                        'mask': tf.ensure_shape(full_comb_mask_store,\n",
    "                                                [output_length-crop_size*2,1]),\n",
    "                        'mask_seed': tf.ensure_shape(mask_seed,\n",
    "                                                [output_length-crop_size*2,1]),\n",
    "                        'mask_gathered': tf.ensure_shape(mask_gathered,\n",
    "                                                [(output_length-crop_size*2)//2,1]),\n",
    "                        'mask_gathered_seed': tf.ensure_shape(masked_gathered_seed,\n",
    "                                                [(output_length-crop_size*2)//2,1]),\n",
    "                        'peaks': tf.ensure_shape(peaks_gathered,\n",
    "                                                  [(output_length-2*crop_size) // 2,1]),\n",
    "                        'target': tf.ensure_shape(atac_out,\n",
    "                                                  [output_length-crop_size*2,1])}\n",
    "\n",
    "\n",
    "            files = tf.data.Dataset.list_files(list_file)\n",
    "\n",
    "            dataset = tf.data.TFRecordDataset(files,\n",
    "                                              compression_type='ZLIB',\n",
    "                                              num_parallel_reads=4)\n",
    "            dataset = dataset.with_options(options)\n",
    "            dataset = dataset.map(lambda record: deserialize_val(record,\n",
    "                                                                262144,\n",
    "                                                                10,\n",
    "                                                                65536,\n",
    "                                                                2048,\n",
    "                                                                256,\n",
    "                                                                128,\n",
    "                                                                 #seq_mask_dropout,\n",
    "                                                                0.10,\n",
    "                                                                1024,\n",
    "                                                                True,\n",
    "                                                               False,\n",
    "                                                               True,\n",
    "                                                                g,h),\n",
    "                          deterministic=False,\n",
    "                          num_parallel_calls=4)\n",
    "\n",
    "            dataset = dataset.batch(64,drop_remainder=True).prefetch(tf.data.AUTOTUNE).repeat()\n",
    "\n",
    "            test_dist = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "            test_it = iter(test_dist)\n",
    "            test_it_build = iter(test_dist)\n",
    "            files_list.append(test_it)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
