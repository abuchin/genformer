{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f3727c-cca0-414c-9bde-b2a3449eaac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import collections\n",
    "import gzip\n",
    "import math\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "#silence_tensorflow()\n",
    "os.environ['TPU_LOAD_LIBRARY']='0'\n",
    "os.environ['TF_ENABLE_EAGER_CLIENT_STREAMING_ENQUEUE']='False'\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import strings as tfs\n",
    "from tensorflow.keras import mixed_precision\n",
    "from scipy.stats.stats import pearsonr  \n",
    "from scipy.stats.stats import spearmanr  \n",
    "## custom modules\n",
    "import src.aformer_TF_gc_separated as aformer\n",
    "#import src.aformer_TF as aformer\n",
    "from src.layers.layers import *\n",
    "import src.metrics as metrics\n",
    "from src.optimizers import *\n",
    "import src.schedulers as schedulers\n",
    "import src.utils as utils\n",
    "\n",
    "import training_utils_aformer_TF_genecentered_separated as training_utils\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554e5bc7-fc53-41ee-8dad-ee3251aab7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-22')\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.FILE\n",
    "    options.deterministic=False\n",
    "    #options.experimental_threading.max_intra_op_parallelism = 1\n",
    "    mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    #options.num_devices = 64\n",
    "\n",
    "    BATCH_SIZE_PER_REPLICA = 8\n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41c81633-2964-456b-8c46-bb7f2f292be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    train_steps=50#3200#5165 #320\n",
    "    warmup_steps=10\n",
    "    val_steps_h=10#3200#757 ### 5562\n",
    "    val_steps_m=14\n",
    "    num_epochs=5\n",
    "    lr_base=0.001\n",
    "    warmup_lr=1.0e-06\n",
    "\n",
    "    data_it_tr_list = []\n",
    "    data_it_val_list = []\n",
    "\n",
    "    ### create dataset iterators\n",
    "    heads_dict = {}\n",
    "    orgs = [\"hg\"]\n",
    "    for k, org in enumerate(orgs):\n",
    "        heads_dict[org] = int(k)\n",
    "    data_dict_tr,data_dict_val = training_utils.return_distributed_iterators(heads_dict,\n",
    "                                                                             \"gs://picard-testing-176520/16k_genecentered_blacklist0.50_atacnormalized/preprocessed\",\n",
    "                                                                              GLOBAL_BATCH_SIZE,\n",
    "                                                                              16384,\n",
    "                                                                              300,\n",
    "                                                                              \"logTPM\",\n",
    "                                                                              16,\n",
    "                                                                              num_epochs,\n",
    "                                                                              strategy,\n",
    "                                                                              options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d8702c-380d-4263-8d95-566615f76ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = aformer.aformer(kernel_transformation=\"softmax_kernel_transformation\",\n",
    "                                dropout_rate=0.25,\n",
    "                                input_length=16384,\n",
    "                                num_heads=4,\n",
    "                                numerical_stabilizer=0.0000001,\n",
    "                                nb_random_features=256,\n",
    "                                hidden_size=256,\n",
    "                                d_model=256,\n",
    "                                norm=True,\n",
    "                                dim=64,\n",
    "                                max_seq_length = 2048,\n",
    "                                rel_pos_bins=2048,\n",
    "                                widening = 2, ## ratio between first and second dense layer units in transformer block\n",
    "                                conv_filter_size_1_seq=15,\n",
    "                                conv_filter_size_2_seq=5,\n",
    "                                conv_filter_size_1_atac=15,\n",
    "                                conv_filter_size_2_atac=5,\n",
    "                                positional_dropout_rate=0.1,\n",
    "                                transformer_depth=2,\n",
    "                                momentum=0.90,\n",
    "                                channels_list=[96,96,112,112,128,128], \n",
    "                                kernel_regularizer=0.0000001,\n",
    "                                bottleneck_units=64,\n",
    "                            bottleneck_units_tf=64,\n",
    "                                use_mask_pos=False,\n",
    "                                use_rot_emb=True,\n",
    "                                heads_dict=heads_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19597c14-40cc-424c-9680-d75516580e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    scheduler= tf.keras.optimizers.schedules.CosineDecay(\n",
    "        initial_learning_rate=5.0e-04,\n",
    "        decay_steps=250, alpha=(5.0e-08 / 5.0e-04))\n",
    "    scheduler=WarmUp(initial_learning_rate=5.0e-04,\n",
    "                                 warmup_steps=50,\n",
    "                                 decay_schedule_fn=scheduler)\n",
    "\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=scheduler,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.999,\n",
    "                                     weight_decay=0.01)\n",
    "\n",
    "    optimizer=tfa.optimizers.Lookahead(optimizer,\n",
    "                                       sync_period=6,\n",
    "                                       slow_step_size=0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b410e6c0-6220-418a-b6e1-868776fc73e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    metric_dict = {}\n",
    "    train_step, val_step, metric_dict = training_utils.return_train_val_functions_hg(model,\n",
    "                                                                                 optimizer,\n",
    "                                                                                 strategy,\n",
    "                                                                                 metric_dict, \n",
    "                                                                                 train_steps,\n",
    "                                                                                 val_steps_h,\n",
    "                                                                                 val_steps_m,\n",
    "                                                                                 GLOBAL_BATCH_SIZE,\n",
    "                                                                                 0.2,\n",
    "                                                                                 True,\n",
    "                                                                                 freq_limit=5000,\n",
    "                                                                                 fourier_loss_scale=1.0,\n",
    "                                                                                 use_tf_acc=False) # last is uncropped length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e58bc6c-aa7a-44a8-b534-4dd4749ebf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d34035-65ed-44e7-9ad4-d6d2d402b0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hg_train_loss: 0.36930645\n",
      "hg_lr: [1.00000007e-05 2.00000013e-05 3.00000029e-05 4.00000026e-05\n",
      " 5.00000024e-05 6.00000058e-05 7.00000019e-05 8.00000053e-05\n",
      " 9.00000086e-05 1.00000005e-04 1.10000008e-04 1.20000012e-04\n",
      " 1.30000015e-04 1.40000004e-04 1.50000007e-04 1.60000011e-04\n",
      " 1.70000014e-04 1.80000017e-04 1.90000006e-04 2.00000009e-04\n",
      " 2.10000013e-04 2.20000016e-04 2.30000020e-04 2.40000023e-04\n",
      " 2.50000012e-04 2.60000030e-04 2.70000019e-04 2.80000007e-04\n",
      " 2.90000025e-04 3.00000014e-04 3.10000032e-04 3.20000021e-04\n",
      " 3.30000010e-04 3.40000028e-04 3.50000017e-04 3.60000035e-04\n",
      " 3.70000023e-04 3.80000012e-04 3.90000030e-04 4.00000019e-04\n",
      " 4.10000037e-04 4.20000026e-04 4.30000015e-04 4.40000033e-04\n",
      " 4.50000021e-04 4.60000039e-04 4.70000028e-04 4.80000046e-04\n",
      " 4.90000006e-04 5.00000024e-04]\n",
      "hg_it: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr  \n",
    "with strategy.scope():\n",
    "    def sum_log(x):\n",
    "        return np.log10(1.0 + np.nansum(x))\n",
    "    \n",
    "    global_step = 0\n",
    "    val_losses = []\n",
    "    val_pearsons = []\n",
    "    val_R2 = []\n",
    "    patience_counter = 0\n",
    "    stop_criteria = False\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch_i in range(1, 15):\n",
    "        start = time.time()\n",
    "        #if epoch_i > 2 : \n",
    "        lr, it = train_step(data_dict_tr['hg'])\n",
    "\n",
    "                   #data_dict_tr['mm'],\n",
    "                   #data_dict_tr['rm'])\n",
    "        print('hg_train_loss: ' + str(metric_dict['hg_tr'].result().numpy()))\n",
    "        \n",
    "        print('hg_lr: ' + str(lr.numpy()))\n",
    "        print('hg_it: ' + str(it.numpy()))\n",
    "        \n",
    "        val_step(data_dict_val['hg'])\n",
    "\n",
    "        val_losses.append(metric_dict['hg_val'].result().numpy())\n",
    "        print('hg_val_loss: ' + str(metric_dict['hg_val'].result().numpy()))\n",
    "        print('hg_val_pearson: ' + str(metric_dict['hg_corr_stats'].result()['pearsonR'].numpy()))\n",
    "        print('hg_val_R2: ' + str(metric_dict['hg_corr_stats'].result()['R2'].numpy()))\n",
    "        \n",
    "        \n",
    "        #print('hg_val_pearson_gene: ' + str(metric_dict['hg_corr_stats'].result()['feature_level_pearsonR'].numpy()))\n",
    "        #print('hg_val_R2_gene: ' + str(metric_dict['hg_corr_stats'].result()['feature_level_R2'].numpy()))\n",
    "        \n",
    "        y_trues = metric_dict['hg_corr_stats'].result()['y_trues'].numpy()\n",
    "        y_preds = metric_dict['hg_corr_stats'].result()['y_preds'].numpy()\n",
    "        cell_types = metric_dict['hg_corr_stats'].result()['cell_types'].numpy()\n",
    "        gene_map = metric_dict['hg_corr_stats'].result()['gene_map'].numpy()\n",
    "\n",
    "        unique_preds = {}\n",
    "        unique_trues = {}\n",
    "        for k,x in enumerate(gene_map):\n",
    "            unique_preds[(cell_types[k],x)] = y_preds[k]\n",
    "            unique_trues[(cell_types[k],x)] = y_trues[k]\n",
    "        \n",
    "        unique_preds = dict(sorted(unique_preds.items()))\n",
    "        unique_trues = dict(sorted(unique_trues.items()))\n",
    "        \n",
    "        \n",
    "        print('overall gene pearsonsR:', pearsonr(y_trues,\n",
    "                                                    y_preds)[0])\n",
    "        \n",
    "        print('overall gene spearmanR:', spearmanr(y_trues,\n",
    "                                                    y_preds)[0])\n",
    "        data = np.vstack([y_trues,y_preds])\n",
    "        kernel = stats.gaussian_kde(data)(data)\n",
    "        sns.scatterplot(\n",
    "            x=y_trues,\n",
    "            y=y_preds,\n",
    "            c=kernel,\n",
    "            cmap=\"viridis\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        ### now compute correlations across cell types\n",
    "        across_cells_preds = {}\n",
    "        across_cells_trues = {}\n",
    "        \n",
    "        for k,v in unique_preds.items():\n",
    "            cell_t,gene_name = k\n",
    "            if cell_t not in across_cells_preds.keys():\n",
    "                across_cells_preds[cell_t] = []\n",
    "                across_cells_trues[cell_t] = []\n",
    "            else:\n",
    "                across_cells_preds[cell_t].append(v)\n",
    "                across_cells_trues[cell_t].append(unique_trues[k])\n",
    "        cell_specific_corrs = []\n",
    "        for k,v in across_cells_preds.items():\n",
    "            trues = []\n",
    "            preds = []\n",
    "            for idx,x in enumerate(v):\n",
    "                preds.append(x)\n",
    "                trues.append(across_cells_trues[k][idx])\n",
    "            try: \n",
    "                cell_specific_corrs.append(pearsonr(trues, \n",
    "                                                    preds)[0])\n",
    "            except np.linalg.LinAlgError:\n",
    "                continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        sns.histplot(x=np.asarray(cell_specific_corrs), bins=50)\n",
    "        plt.show()\n",
    "        print('median_cell_crossgenes:', np.nanmedian(cell_specific_corrs))\n",
    "                \n",
    "            \n",
    "        ### now compute correlations across genes\n",
    "        across_genes_preds = {}\n",
    "        across_genes_trues = {}\n",
    "        \n",
    "        for k,v in unique_preds.items():\n",
    "            cell_t,gene_name = k\n",
    "            if gene_name not in across_genes_preds.keys():\n",
    "                across_genes_preds[gene_name] = []\n",
    "                across_genes_trues[gene_name] = []\n",
    "            else:\n",
    "                across_genes_preds[gene_name].append(v)\n",
    "                across_genes_trues[gene_name].append(unique_trues[k])\n",
    "        genes_specific_corrs = []\n",
    "        genes_specific_vars = []\n",
    "        for k,v in across_genes_preds.items():\n",
    "            trues = []\n",
    "            preds = []\n",
    "            for idx, x in enumerate(v):\n",
    "                #if len(x) > 0:\n",
    "                preds.append(x)\n",
    "                trues.append(across_genes_trues[k][idx])\n",
    "            try: \n",
    "                genes_specific_corrs.append(spearmanr(trues, \n",
    "                                                     preds)[0])\n",
    "                genes_specific_vars.append(np.var(trues))\n",
    "            except np.linalg.LinAlgError:\n",
    "                continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        sns.histplot(x=np.asarray(genes_specific_corrs), bins=50)\n",
    "        plt.show()\n",
    "        print('median_gene_crossdataset:', np.nanmedian(genes_specific_corrs))\n",
    "            \n",
    "        sns.scatterplot(\n",
    "            x=genes_specific_vars,\n",
    "            y=genes_specific_corrs)\n",
    "        plt.show()\n",
    "\n",
    "        #print('mm_train_loss: ' + str(metric_dict['mm_tr'].result().numpy()))\n",
    "\n",
    "        #print('mm_val_loss: ' + str(metric_dict['mm_val'].result().numpy()))\n",
    "        #print('mm_val_pearson: ' + str(metric_dict['mm_corr_stats'].result()['pearsonR'].numpy()))\n",
    "        #print('mm_val_R2: ' + str(metric_dict['mm_corr_stats'].result()['R2'].numpy()))\n",
    "        \n",
    "        end = time.time()\n",
    "        duration = (end - start) / 60.\n",
    "        print('completed epoch ' + str(epoch_i))\n",
    "        print('duration(mins): ' + str(duration))\n",
    "        print('patience counter at: ' + str(patience_counter))\n",
    "\n",
    "        for key, item in metric_dict.items():\n",
    "            item.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d71db5-6c78-4930-b4d1-d1e94a759e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.lr(optimizer.iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf8deb8-73b7-43ec-a25a-fe4da484a2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy.reduce(\"SUM\",\n",
    "                optimizer.iterations, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce302afd-e17b-4327-a4e6-9fcc360b18a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "op"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
