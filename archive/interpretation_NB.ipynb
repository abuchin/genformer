{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b7e921-ad44-4d66-bc22-8fe68287529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "import argparse\n",
    "import collections\n",
    "import gzip\n",
    "import math\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import logging\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "#silence_tensorflow()\n",
    "os.environ['TPU_LOAD_LIBRARY']='0'\n",
    "os.environ['TF_ENABLE_EAGER_CLIENT_STREAMING_ENQUEUE']='False'\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import strings as tfs\n",
    "from tensorflow.keras import mixed_precision\n",
    "from scipy.stats.stats import pearsonr  \n",
    "from scipy.stats.stats import spearmanr  \n",
    "## custom modules\n",
    "import src.aformer_TF_gc_separated as aformer\n",
    "#import src.aformer_TF as aformer\n",
    "from src.layers.layers import *\n",
    "import src.metrics as metrics\n",
    "from src.optimizers import *\n",
    "import src.schedulers as schedulers\n",
    "import src.utils as utils\n",
    "\n",
    "import training_utils_aformer_TF_genecentered_separated as training_utils\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa48769-dc24-411a-99b0-bfcbc62a0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-15')\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.FILE\n",
    "    options.deterministic=False\n",
    "    #options.experimental_threading.max_intra_op_parallelism = 1\n",
    "    mixed_precision.set_global_policy('mixed_bfloat16')\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    #options.num_devices = 64\n",
    "\n",
    "    BATCH_SIZE_PER_REPLICA = 1\n",
    "    NUM_REPLICAS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * NUM_REPLICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8e093-4c3e-4b33-b1f4-cbedc67f77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    heads_dict = {}\n",
    "    orgs = [\"hg\",\"mm\"]\n",
    "    for k, org in enumerate(orgs):\n",
    "        heads_dict[org] = int(k)\n",
    "    model = aformer.aformer(kernel_transformation=\"relu_kernel_transformation\",\n",
    "                                dropout_rate=0.35,\n",
    "                                input_length=16384,\n",
    "                                num_heads=4,\n",
    "                                numerical_stabilizer=0.0000001,\n",
    "                                nb_random_features=128,\n",
    "                                hidden_size=128,\n",
    "                                d_model=128,\n",
    "                                norm=True,\n",
    "                                dim=32,\n",
    "                                max_seq_length = 128,\n",
    "                                rel_pos_bins=512,\n",
    "                                widening = 2, ## ratio between first and second dense layer units in transformer block\n",
    "                                conv_filter_size_1_seq=15,\n",
    "                                conv_filter_size_2_seq=5,\n",
    "                                conv_filter_size_1_atac=15,\n",
    "                                conv_filter_size_2_atac=5,\n",
    "                                positional_dropout_rate=0.1,\n",
    "                                transformer_depth=1,\n",
    "                                momentum=0.90,\n",
    "                                channels_list=[48,48,56,56,64,64], \n",
    "                                kernel_regularizer=0.0000001,\n",
    "                                bottleneck_units=32,\n",
    "                            bottleneck_units_tf=32,\n",
    "                                use_mask_pos=False,\n",
    "                                use_rot_emb=True,\n",
    "                                heads_dict=heads_dict)\n",
    "    model.load_weights(\"gs://picard-testing-176520/16k_genecentered_blacklist0.50_atacnormalized/models/aformer_TF_gene_centered_test/final/saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45150be-7770-4ef5-92a9-386b141b6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    gcs_path = \"gs://picard-testing-176520/test_gene/test_gene.tfr\"\n",
    "    val_data = training_utils.return_dataset_interpret(gcs_path,\n",
    "                                                       strategy,\n",
    "                                                         1,\n",
    "                                                         16384,\n",
    "                                                         \"logTPM\",\n",
    "                                                         4,\n",
    "                                                         10,\n",
    "                                                         1637)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb78c0c-24f7-4634-bdb3-ea4779e0e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    model = aformer.aformer(kernel_transformation=\"relu_kernel_transformation\",\n",
    "                                dropout_rate=0.35,\n",
    "                                input_length=16384,\n",
    "                                num_heads=4,\n",
    "                                numerical_stabilizer=0.0000001,\n",
    "                                nb_random_features=128,\n",
    "                                hidden_size=128,\n",
    "                                d_model=128,\n",
    "                                norm=True,\n",
    "                                dim=32,\n",
    "                                max_seq_length = 128,\n",
    "                                rel_pos_bins=512,\n",
    "                                widening = 2, ## ratio between first and second dense layer units in transformer block\n",
    "                                conv_filter_size_1_seq=15,\n",
    "                                conv_filter_size_2_seq=5,\n",
    "                                conv_filter_size_1_atac=15,\n",
    "                                conv_filter_size_2_atac=5,\n",
    "                                positional_dropout_rate=0.1,\n",
    "                                transformer_depth=1,\n",
    "                                momentum=0.90,\n",
    "                                channels_list=[48,48,56,56,64,64], \n",
    "                                kernel_regularizer=0.0000001,\n",
    "                                bottleneck_units=32,\n",
    "                            bottleneck_units_tf=32,\n",
    "                                use_mask_pos=False,\n",
    "                                use_rot_emb=True,\n",
    "                                heads_dict=heads_dict)\n",
    "    model.load_weights(\"gs://picard-testing-176520/16k_genecentered_blacklist0.50_atacnormalized/models/aformer_TF_gene_centered_test/final/saved_model\")\n",
    "\n",
    "\n",
    "    def predict_on_batch(model, inputs):\n",
    "        return model.predict_on_batch(inputs)\n",
    "\n",
    "    @tf.function\n",
    "    def contribution_input_grad(model, model_inputs,output_head='hg'):\n",
    "        seq, atac, tf_acc=model_inputs\n",
    "\n",
    "        with tf.GradientTape() as input_grad_tape:\n",
    "            input_grad_tape.watch(seq)\n",
    "            input_grad_tape.watch(atac)\n",
    "            input_grad_tape.watch(tf_acc)\n",
    "            inputs = seq,atac,tf_acc\n",
    "            prediction = model.predict_on_batch(inputs)[0][output_head]\n",
    "\n",
    "\n",
    "        input_grads = input_grad_tape.gradient(prediction,inputs)\n",
    "        \n",
    "\n",
    "        input_grads_seq = input_grads[0] \n",
    "        input_grads_atac = input_grads[1]\n",
    "        input_grads_tf_acc = input_grads[2]\n",
    "        \n",
    "        \n",
    "        seq_grads = tf.reduce_sum(input_grads_seq[0,:,1:] * test_input['inputs'].values[0][0,:,1:],\n",
    "                                  axis=1)\n",
    "        \n",
    "        tss_grads = input_grads_seq[0,:,0]\n",
    "        \n",
    "        atac_grads = input_grads_atac[0,:,] * test_input['atac'].values[0][0,:,]\n",
    "        \n",
    "        tf_acc_grads = input_grads[2][0,:]\n",
    "        \n",
    "        return seq_grads, tss_grads, atac_grads, tf_acc_grads,prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161807df-12fc-4c0a-b248-c18ca100e5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    test_input = next(val_data)\n",
    "    inputs = test_input['inputs'].values[0],test_input['atac'].values[0], test_input['TF_acc'].values[0]\n",
    "    scores = contribution_input_grad(model,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57190c99-23ef-4a45-a1b6-d2355b42606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_track_seq(tss_tokens, input_arr, start, stop, height=1.5):\n",
    "    length=stop-start\n",
    "\n",
    "\n",
    "    # Set y-limit, making neg y-values not show in plot\n",
    "    #plt.ylim(start, stop)\n",
    "    x_vals = np.linspace(start,stop,num=length)\n",
    "    baseline = np.zeros_like(x_vals)\n",
    "    # Filling between line y3 and line y4\n",
    "    plt.fill_between(x_vals, baseline, input_arr[start:stop],alpha=0.9)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f24bd-85ab-4ec0-bb05-a3b57be7b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_tokens = test_input['tss_tokens'].values[0]\n",
    "plot_track_seq(tss_tokens,scores[0],0,16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f5d72e-4f5c-4b58-915e-bf4fa513a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_track_seq(tss_tokens,tf.abs(scores[0]),0,16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51128d5-91d1-4281-8d52-7aa2e2f32ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((tf.abs(scores[1][tf.newaxis])), aspect = \"auto\", cmap=\"viridis\")\n",
    "plt.gca().set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83634d3e-7fec-47e5-9f84-21e0996ac7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#plot_track_seq(tss_tokens,test_input['atac'].values[0][0,:,0],0,16384)\n",
    "\n",
    "\n",
    "plt.imshow(tf.transpose(tf.abs(scores[2])), aspect = \"auto\", cmap=\"viridis\")\n",
    "plt.gca().set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e0519-c5fe-441e-906a-642de76ca9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_track_seq(tss_tokens,test_input['atac'].values[0][0,:,0],0,16384)\n",
    "\n",
    "\n",
    "plt.imshow(tf.transpose(test_input['atac'].values[0][0,:]), aspect = \"auto\", cmap=\"viridis\")\n",
    "plt.gca().set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb92c1-af20-4097-a4aa-3792736f7509",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tss_tokens[np.newaxis], aspect = \"auto\", cmap=\"viridis\")\n",
    "plt.gca().set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90252b5a-32ff-40a9-bdab-7815dbe0f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_input['inputs'].values[0],test_input['atac'].values[0], test_input['TF_acc'].values[0]\n",
    "att_matrices = model.predict_on_batch(inputs)[1]\n",
    "k_1,q_1 = att_matrices['layer_0']\n",
    "mat = tf.nn.softmax((k_1[:,0,:,:] * tf.transpose(q_1[:,0,:,:])) / tf.math.sqrt(128.0))\n",
    "\n",
    "mat_ave = sum([mat[:,0,k,:] for k in range(4)]) / 4.0\n",
    "\n",
    "plt.matshow(mat_ave)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
